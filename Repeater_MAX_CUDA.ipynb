{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Repeater_MAX_CUDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "epF1okM-Yj5q",
        "outputId": "999df9e7-be3a-4880-b01a-11978feaab1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y08mXtN0Ylcd",
        "outputId": "1367bcd5-f611-45ee-a92a-e5e7ac267b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Nov  7 07:26:09 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7LjR_tAYlmO",
        "outputId": "c52c3e85-a6ee-41cd-a94a-83d585c96273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!gdown --id 1Iy1e6nrpYKUhfIVrABt9E0_IX2xAUmvh"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Iy1e6nrpYKUhfIVrABt9E0_IX2xAUmvh\n",
            "To: /content/intention_repeater_max_cuda.cu\n",
            "\r  0% 0.00/13.4k [00:00<?, ?B/s]\r100% 13.4k/13.4k [00:00<00:00, 6.27MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZSZ8QdfYluN"
      },
      "source": [
        "!nvcc intention_repeater_max_cuda.cu -O3 -o intention_repeater_max_cuda"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pHxDYDqYl0c",
        "outputId": "ed9909b7-0dee-45e9-eca2-87dc4c942eb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/content/intention_repeater_max_cuda --help"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intention Repeater MAX CUDA v2.0 (c)2020 Thomas Sweet aka Anthro Teacher.\n",
            "CUDA and flags functionality by Karteek Sheri.\n",
            "Intention multiplying functionality by Thomas Sweet.\n",
            "\n",
            "Optional Flags:\n",
            "\ta) --gpu or -g\n",
            "\tb) --dur or -d\n",
            "\tc) --rate or -r\n",
            "\td) --imem or -m\n",
            "\te) --intent or -i\n",
            "\tf) --help\n",
            "\n",
            "--gpu = GPU # to use. Default = 0.\n",
            "--dur = Duration in HH:MM:SS format. Example 00:01:00 to run for one minute. Default = \"Until Stopped.\"\n",
            "--rate = Specify Average or Realtime frequency update. Default = Average. Average is faster, but Realtime more accurately reflects each second.\n",
            "--imem = Specify how many GB of GPU RAM to use. Default = 1.0. Higher amount produces a faster repeat rate, but takes longer to load into memory.\n",
            "--intent = Intention. Default = Prompt the user for intention.\n",
            "--help = Display this help.\n",
            "\n",
            "Example automated usage: intention_repeater_max_cuda.exe --gpu 0 --dur 00:01:00 --rate Average --imem 1.0 --intent \"I am calm.\"\n",
            "Default usage: intention_repeater_max_cuda.exe\n",
            "\n",
            "gitHub Repository: https://github.com/tsweet77/repeater-max-cuda\n",
            "Forum: https://forums.intentionrepeater.com\n",
            "Website: https://www.intentionrepeater.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsBjKi_JZH72",
        "outputId": "68a13773-d544-49cb-9356-b43f976caab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/content/intention_repeater_max_cuda --gpu 0 --dur 00:00:01 --rate Average --imem 10 --intent \"I am calm.\""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intention Repeater MAX CUDA v2.0 created by Thomas Sweet.\n",
            "CUDA and flags functionality by Karteek Sheri.\n",
            "Intention multiplying functionality by Thomas Sweet.\n",
            "This software comes with no guarantees or warranty of any kind and is for entertainment purposes only.\n",
            "Press Ctrl-C to quit.\n",
            "\n",
            "Loading intention into memory.tcmalloc: large alloc 2013274112 bytes == 0x559a46764000 @  0x7f81f6f7f887 0x7f81f66bab8b 0x7f81f66bc133 0x5599cc1cf902 0x7f81f5c10b97 0x5599cc1d06ea\n",
            "tcmalloc: large alloc 4026540032 bytes == 0x559abe766000 @  0x7f81f6f7f887 0x7f81f66bab8b 0x7f81f66bc133 0x5599cc1cf902 0x7f81f5c10b97 0x5599cc1d06ea\n",
            "tcmalloc: large alloc 2684354560 bytes == 0x5599ce7c4000 @  0x7f81f6f7f887 0x7f81f66bd2ad 0x7f81f66bd47a 0x5599cc1cf952 0x7f81f5c10b97 0x5599cc1d06ea\n",
            "tcmalloc: large alloc 5368709120 bytes == 0x559a6e7c4000 @  0x7f81f6f7f887 0x7f81f66bab8b 0x7f81f66bc133 0x5599cc1cf98c 0x7f81f5c10b97 0x5599cc1d06ea\n",
            "\n",
            "GPU 0 is selected.\n",
            "tcmalloc: large alloc 2684354560 bytes == 0x5599ce772000 @  0x7f81f6f7d1e7 0x5599cc1cf9b6 0x7f81f5c10b97 0x5599cc1d06ea\n",
            "[00:00:01] (396.3q/396.3PHz): I am calm.     \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}